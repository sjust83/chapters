# In god we trust, all others bring data – BIG DATA!

Markku Oivo  
Dept of Information Processing Science  
University of Oulu, Finland

The legendary statement of the quality guru W. Edwards Deming has been used and quoted countless times in motivating and justifying the role of data in software process assessment and improvement. It is now more relevant than ever, but perhaps with a twist towards data science. Deming was promoting statistical process control. His thinking in the famous PDCA (Plan-Do-Check-Act) is based on planning improvement actions, implementing them, measuring the effect and then acting accordingly. In PDCA we pre-define the actions and consequently pre-define what we want to measure. This pre-defined data is used to guide improvement actions. The fundamental change that data science can bring is that we are able to analyze large amounts of data and draw conclusions based on the data without having pre-planned the data collection specifically for pre-defined actions or items. 

Software process improvement (SPI) has been an active research area since 1980’s and companies have used it a lot in practice. There has been a multitude of SPI methods and solution proposals. The research is has been focusing very much on SPI solution proposals and we are still looking for hard evidence of the effect of our SPI actions.

Assessment based approaches have been popular in SPI ever since the introduction of the SEI’s CMM (Capability Maturity Model) in late 1980’s. The US Department of Defense (DoD) has been a strong supporter of the CMM and its successor CMMI (Capability Maturity Model Integration). A European version called Bootstrap was developed in 1990’s and both models influenced the development of an international standard for process assessment and improvement (ISO/IEC 15504) in an international SPICE project. Automobile industry has been an active user of ISO/IEC 15504 with its own version of it (AutoSPICE). 

The basic idea of assessment methods is to use a reference model to which the processes of the organization are compared. The process capability or organizational maturity is determined based on an assessment of the processes of the organization. Improvement recommendations are given based on the assessment results aiming at higher capability and maturity levels as defined in the model. The whole process is based on data collection mainly from documentation and interviews during an assessment of the organization. The data collection is pre-defined by the process model and practices of the assessment method.

There have been some research efforts as early as in 1990’s trying to integrate assessments with measurement in order to automate at least part of the data collection and enabling a continuous assessment that could replace or complement regular assessments that are done only after relatively long intervals. One of the key problems of automatic data gathering is that there is a lot of data and it is unstructured, not complete and often hard to get. Assessments need precise and complete data. Strikingly - does this sound familiar to data scientists who are used to work with large amounts data that is unstructured, not perfect and not precise? Still they are able to analyze the big data and draw useful conclusions. 

So why not used data sciences in assessment? Would it be enough for process improvement to have the big picture in an assessment that is based on analysis of unstructured big data to complement traditional assessment that digs into the nitty gritty  details of every project that happens to be in the focus of the assessment. Or would it be even better if all the projects would be in the focus of an assessment using data sciences rather than having a sample of a few projects that is analyzed thoroughly? Could we replace the traditional aim for accuracy using a (small) sample in the assessment with a company-wide coverage of all projects with big data approaches? Would the results actually be more accurate this way?

The traditional problem with SPI is lack of evidence of the effect of our improvement actions. It is very hard if not impossible to show with hard evidence what are the benefits of certain SPI actions. Real life is complicated and the effects of SPI are mixed with a myriad other events in the companies. Proving causality of SPI actions and quantitative improvements is extremely tough. Finding correlations is often the only thing we can do. But wait a minute! Isn’t this exactly what data analytics is doing? Could SPI learn something from data sciences? Would it be sometimes ok to find good enough correlations rather than trying to find the evasive accurate causations of SPI actions? Would it be enough to know “what” is happening or predict what will happen after improvement actions, rather than trying to accurately know “why” it is happening?

There is already a wealth of experience in using data sciences in business intelligence. Big data may even be considered as a hype term. Data sciences has also been used in business process improvement. SPI has a lot of similarities with business process improvement and can learn a lot from the experiences in business processes.

Software development produces a lot of data from tools like issue tracking systems, version control systems, test data, code, documents, etc. The amount of data explodes with recording keyboards, voice recording, and video recording. This kind of data may not have any structure and can include just about anything, but it is potentially useful for big data analytics. Surely we will face similar privacy issues as with most big data applications that use personal data. Despite the challenges the payback may be considerably high.

Another area where data sciences has not yet reached its potential is use of customer data to improve the products and software development processes. Game industry and Internet companies are pioneering these approaches. By launching games or services in limited areas they quickly collect usage data and user experiences to improve their products and software processes.

Traditional SPI tries to define the processes in order to optimize them. However, it is often challenging to distinguish between official processes and actual processes. An alternative is to deduce the software processes from the data gathered from actual software development resulting in a description of the actual process – not just the official or desired process. This data can also be used to analyze the workflows and to identify bottlenecks. 

Many of the SPI methods are mainly based on analyzing the past and making an assessment and improvement recommendations based on that analysis. However, what we would really need is to know the future. We would like to predict what will happen in our future endeavors or estimate the effort required for software development. This is where predictive analytics and estimation methods come into play. We already have a growing community of researchers that work on analyzing large amounts of software engineering data in order to learn from it and to predict what will happen in the software development if we take certain actions. These analysis methods go far beyond the simple post mortem analysis.

An interesting trend is use of large cross-project and open source repositories for analyzing software engineering data and drawing conclusions from that data. Several such repositories have emerged. One example of the promising repositories is the PROMISE dataset (http://openscience.us/repo/) that includes data from hundreds of projects. It aims to serve as a long-term repository for software engineering data that researchers around the world can use. Good examples of the use of such data include defect and cost modeling that are used for prediction and estimation.

