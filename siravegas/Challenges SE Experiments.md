#Challenges of Conducting Software Engineering Experiments: How to Stop Things Going Wrong###Sira Vegas & Natalia Juristo

An astronomer wants to buy a telescope to observe a distant galaxy. While he is unsure about precisely what features the telescope he needs should have, he does know exactly how much he has to spend on equipment. So, without any further analysis of the required specifications, he goes ahead and orders the best telescope that he can afford and hopes for the best. But, viewed through the wrong telescope, the galaxy will be an indecipherable blur, and the equipment will be useless. Several factors regarding telescope design should to be taken into account (whether the objective should be a lens or a mirror, the diameter of the objective, the quality of the lens/mirrors, etc.). Apart from its price, they affect its magnification or power. In other words, what we will be able to observe through an instrument depends on the instrument’s characteristics.   
An experiment is an instrument that we use in SE to observe software development. If our instrument is not properly aligned everything will be a blur, but we will mistakenly take it to be right. The reliability of the findings is critically dependent on the quality of the instrument, the alignment between the instrument and the phenomenon that we are studying.  
As in any other discipline, conducting laboratory experiments in software engineering (SE) is a challenging error-prone activity. Other fields are tackling the issue of how much trust they can place in experiment results. Pashler and Wagenmakers report “a crisis of confidence in psychological science reflecting an unprecedented level of doubt among practitioners about the reliability of research findings in the field”. The reliability of the results is highly dependent on design and protocol quality. Not everything goes.  
Experimentation is a fairly recent practice in SE compared with other much more mature experimental disciplines. Experimentalism is a paradigm that needs to be instantiated, translated and adapted to the idiosyncrasy of each experimental discipline. Copy and paste, that is, copying from physics what an experiment is, copying from medicine the threats to the validity of experiments, or copying from psychology how to deal with experimental subjects, will not do at all. We can borrow from all these experiences, but our discipline needs to adopt its own form of experimentalism. We all need to learn more, and much more effort and research is needed to adapt the experimental paradigm to SE.  
Based on our experiences of running experiments and reading the reports on other experiments, we have spotted some common mistakes in SE experiments. As a result, we have identified some good practices that may be helpful for avoiding common pitfalls.  
**Operationalize constructs.** Operationalization describes the act of translating a construct into its manifestation. In a controlled experiment, we have cause and effect constructs, which are operationalized as treatments and dependent variables, respectively. For cause constructs, it is necessary to specify how the treatment will be applied, as SE’s immaturity (which very often shows up as informality) might lead different people to interpret the treatment differently. Effect constructs should take into account not only the metrics used to measure the dependent variable, but also the measurement procedure. Again, the measurement procedure in SE is very often non-standard and requires specification.  
**Evaluate different design alternatives.** The simpler the design is, the better it will be. One-factor between-subjects experiments are the most manageable. Because of SE’s intrinsic properties, however, they are very often not the right choice in this field. 
 * Small sample sizes and a large variability in subject behaviour make it almost impossible to use between-subjects designs in experiments run with subjects. 
* The influence of the intrinsic properties of the experimental objects (programs/software systems/tasks) and subjects (if any) very often obliges researchers to perform stratified randomization or blocking.
* Software development process complexity makes it more or less impossible for blocking to rule out the influence of other sources of variability, and more than one factor needs to be added to the design.Consequently, experimental design has to be approached iteratively, trying out different designs and then analysing trade-offs.  
**Match data analysis and experimental design.** Data analysis is driven by the experimental design. Issues such as the metric used to measure the treatments and dependent variables, the number of factors, whether the experiment has a between- or within–subjects design, and the use of blocking variables will all determine the particular data analysis technique to be applied. This data analysis technique maps the design to the statistical model in terms of the factors and interactions to be analysed. However, the choice of data analysis technique and/or statistical model is not always as straightforward as all that:  
* Parametric tests are the preferred option, as they are more powerful than non-parametric tests and are capable of analysing several factors and their interactions. But the data do not always meet the data analysis technique requirements (normality of the data or residuals and/or homogeneity of variances depending on the technique in question). An alternative to non-parametric tests is data transformation. Additionally, some tests are robust to deviations from normality.
* Complex designs may require the addition of some extra factors to the statistical model. Take, for example, crossover designs, where each experimental subject applies all treatments, but different subjects apply treatments in a different order. The order in which subjects apply treatments (sequences) and the times at which each treatment is applied (periods) have to be added to the factor analysis, and a decision has to be made about how to deal with carryover.  
**Do not rely on statistical significance alone.** All experiments report statistical significance. However, statistical significance is the probability of observing an effect given that the null hypothesis is true. In other words, it measures whether the observed effect really is caused by the population characteristics or is merely the result of sampling error. But it gives no indication of how big the difference in treatments is. For relatively large sample sizes, even very small differences may be statistically significant. On this ground, we need a measure of practical significance. The question is whether the differences between treatments are large enough to be really meaningful. This is generally assessed using a measure of effect size. There is a wide range of over 70 effect size measures, capable of reporting different types of effects.
 **Do a power analysis.** Power analysis can be done before (a priori) or after (post-hoc). A priori power analysis tells experimenters what minimum sample size they need to have a reasonable chance of detecting an effect of a given size before they run the experiment. Of course, experimenters will require a bigger sample size if they are looking for small effects than to detect medium or big effects. Post-hoc power analysis determines the power of the study assuming that the sample effect size is equal to the population effect size. While the utility of a priori power analysis is universally accepted, the usefulness of post-hoc power analysis is controversial, as it is a one-to-one function of the statistical significance.  
**Find explanations for results.** The goal of the experiment is to answer the research questions. Experimenters should not, therefore, just stop when the null hypothesis is (not) rejected; they should question why they got the results that they got. They should hypothesize why one treatment is (or is not) better than the other and what might be causing differences in behaviour.  
 **Follow guidelines for reporting experiments.** The way in which an experiment and its results are reported is just as important as the actual results.  Running SE experiments is a multifaceted process. Many different issues must be taken into account. Moreover, software engineering has some special features, leading to some issues concerning experimentation being conceived of differently than in other disciplines.  To know more* P.D. Ellis. The Essential Guide to Effect Sizes. Statistical Power, Meta-analysis and the Interpretation of Research Results. Cambridge University Press. 2010.
* A. Jedlitschka, M.  Ciolkowski, D. Pfahl. Reporting Controlled Experiments in Software Engineering; in Shull, F.; Singer, J.; Sjoberg, D.I. (eds.).; Guide to Advanced Empirical Software Engineering; Springer 2008.
* N. Juristo, A.M. Moreno. Basics of Software Engineering Experimentation. Kluwer, 2001.
* H. Pashler and E.-J. Wagenmakers, “Editors’ Introduction to the Special Section on Replicability in Psychological Science: A Crisis of Confidence?” Perspectives on Psychological Science, vol. 7, no. 6, pp. 528–530, 2012.
* C. Wohlin, P. Runeson, M. Höst, M.C. Ohlsson, B. Regnell, A. Wesslén. Experimentation in Software Engineering. Springer 2012. 