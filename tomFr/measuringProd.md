# Measuring individual productivity

In the last century, one company was said to have determined the price of a software product by the estimated number of lines of code necessary to be written. In turn the company also paid their software developers based on the number of lines of code produced per day. What happened next is that the company's developers started 'gaming the system'--they wrote all of a sudden a lot more code while at the same time the functionality captured in the code decreased. As one might imagine, adding more lines of code to a program without changing the behavior is easy. So  incentivizing employees on a single outcome metric, might just foster this. 

Overall, this example shows that it is not easy to find a good measure for a developer's success or productivity. In some domains, such as car manufacturing, specific measures on the quantity of the outcome, such as the number of cars produced in a day, have worked well to incentivize employees and to measure their success. In the software development domain, however, where the outcome and the overall process of developing software is less clearly defined and more flexible, such outcome measures are difficult to define. In particular, trying to reduce the complex activity of developing software into a single measure of one particular outcome of the development process is probably impossible.

## No single and simple best metric for success/productivity
In a study we conducted, we asked professional developers how they measure and assess their personal  productivity [Meyer2014]. The results show that software developers generally want a combination of measures to assess productivity and these combinations varied significantly across developers. Even for the one metric that developers rated highest overall for assessing productivity---the number of work items (tasks, bugs) closed---developers stated that it heavily depends on the complexity and size of the task, so that further information is needed to interpret this metric.

These findings further indicate that there is no single and simple best measure for assessing a developer's productivity. Given the variety of artifacts produced during software development, such as code artifacts, test cases, bug reports, and documentation, it is also not surprising that just focusing on the code would reflect the progress a developer makes in his/her work. As one example of the variety of artfiacts generated by developers every day, are the around 10 thousand Java source files, 26 thousand Bugzilla bug reports and the 45 thousand newsgroup entries that were created over one development cycle of Eclipse--an open source IDE [EclipseStats]. While the code is the one that is being compiled and executed in the end, the other artifacts are just as important in the process to make sure the software product is developed the right way and works.

## Measure the process, not just the outcome
The variety in outcomes or artifacts generated in the process is just one important aspect for measuring a developer's work and productivity. A majority of the participants in our study also mentioned that they are particularly productive when they get into the "flow" without many context-switches. So rather than just focusing on a measure of the outcomes of the development activity, such as the code artifacts, the  process of developing the software  is important as well. Software development is a complex process that is comprised of various activities and frequent interruptions and interactions with multiple stakeholders involved in the development process, such as fellow developers, requirements engineers, or even customers [Perry1994, Singer2010, Müller2013]. 

Measuring aspects of the process, such as flow or context switches, especially their impact on a developer's productivity is difficult since their impact can vary and is not easy to determine. For example, take a developer who is testing a system and has to wait for a build or the application to start up. Switching context in this situation to perform a shorter task and filling the time in between might actually increase his/her productivity. On the other hand, take a developer who is programming and in the "flow". When the developer is externally interrupted by another developer and asked about last weekends football scores, the forced context switch is expensive, decreases productivity and can even result in more errors being made overall as studies have shown [Bailey2006]. So overall, while aspects of the process are important for measuring productivity, they are difficult to quantify and measure. Recently emerging biometric sensing technologies might provide new means to measure such aspects of individual productivity better, especially due to their pervasiveness and their decreasing invasiveness.

## Allow for measures to evolve
When it comes to a person's fitness and health, wearable fitness tracking devices, such as the Fitbit [Fitbit] or the Withings Pulse Band [Withings] have recently gained widespread adaption. Most of these devices employ a simple step count measure that has also been shown to be very successful in providing users valuable insights on their activity level and in promoting and improving health and fitness (e.g. [Bravata2007, Consolvo2008]). In an interview-based study with 30 participants who had used and adopted wearable activity tracking devices "in the wild" for between 3 and 54 months, we also found that the step count provided long-term motivation for people to become and stay active. Yet, while for several participants the devices helped foster engagement in fitness, the device and in particular the step count measure did not support their increasingly sophisticated fitness goals. As one example, the step count helped some participants to walk more and start running, but when they started adding new activities to further increase fitness, such as weight-lifting or Yoga, the devices failed to capture these [Fritz2014].

Similarly, the activities and responsibilities of an individual developer evolve over time and thus the measures capturing his/her productivity have to evolve. For example, people might start out as a developer in a team and later on be promoted to manage teams. While in the first role, the number of bug fixes completed might be a reasonable proxy for their productivity, as a manager they will focus mostly on their team to be productive and have little to no time to work on bug fixes themselves. 

## Goodhart's law and the effect of measuring
Goodhart's law states that "When a measure becomes a target, it ceases to be a good measure.". This effect happened in the first example presented in this chapter. As soon as the lines of code was used to measure a developer's productivity and affected the developer's salary, it ceased to be a good indicator with developers gaming the system to benefit. Coming back to the fitness-tracking domain, we found from our interviews that the numerical feedback provided by the devices effected users' behavior in ways other than the intended fitness improvement. In many cases, the accountability and getting credit for activities became important and even more important than the original goal, so that users adjusted their sports activities for better accountability, became unhappy and annoyed when they forgot their devices despite being very active, or were merely "fishing for numbers" or gaming  the system. One user, for example, stopped going backwards on an elliptical machine since the device did not pick up on it.

The effect depends on how  and what a measure is being used for, e.g. is it just used for personal retrospection or is it used for adjusting someone's salary. In any case though, one needs to assess the influence a certain measure might have on a developer's behavior and the risks it bares.

## How to measure individual productivity?
Measuring productivity of individual developers is challenging given the complex and multi-facetted nature of developing software. There is not a single and simple best measure that works for all. Rather you will have to tailor the measuring to the specific situation and context you are looking at and the specific goal you have in mind with your measurement. For example, if you are conducting a research study on the productivity gain of a code navigation support tool, a measure such as the edit ratio [Kersten2006], i.e. the ratio between edit and select events, might be a good indicator. However, this neglects other aspects such as interruptions that occur or the resumption lag, which might be important measures when looking at a more general developer productivity definition. In addition, one has to assess who will or should have access to the measurement data and how might this affect the developer's behavior and thus the productivity measure in the end. Finally, a very important point is the privacy of the collected data. Are you actually able to collect the data needed from the developers, are you allowed to analyze it and did you make sure that only the intended people have access to it? Since any productivity measure will most likely require fairly sensitive information that could be used for or against someone, you need to make sure to pay attention to privacy concerns and treat them carefully. 

 
## References

**[Bailey2006]** On the need for attention-aware systems: Measuring effects of interruption on task performance, error rate, and affective state. B. P. Bailey, J. A. Konstan. Computers in Human Behavior 22, 4 (2006). 

**[Bravata2007]** Using pedometers to increase physical activity and improve health. Bravata, D., Smith-Spangler, C., Sundaram, V., Gienger, A., Lin, N., Lewis, R., Stave, C., Olkin, I., Sirard, J. JAMA 2007, 298 

**[Consolvo2008]** Activity sensing in the wild: a field trial of UbiFit garden. Consolvo, S., McDonald, D., Toscos, T., Chen, M., Froehlich, J., Harrison, B., Klasnja, P., LaMarca, A., LeGrand, L., Libby, R., Smith, I., Landay, J. Proc. CHI?08. 

**[EclipseStats]** [http://www.eclipse.org/eclipse/development/eclipse_3_0_stats.html](http://www.eclipse.org/eclipse/development/eclipse_3_0_stats.html)

**[Fitbit**] [http://www.fitbit.com/](http://www.fitbit.com/)

**[Fritz2014]** Persuasive Technology in the Real World: A Study of Long-Term Use of Activity Sensing Devices for Fitness. T. Fritz, E. M. Huang, G. C. Murphy, T. Zimmermann. Proc. of the ACM Conference on Human Factors in Computing Systems 2014 (CHI'14). 

**[Kersten2006]** Using Task Context to Improve Programmer Productivity. M. Kersten, G. C. Murphy. In Proc. of the ACM SIGSOFT 14th International Symposium on the Foundations of Software Engineering 2006 (FSE'06).

**[Meyer2014]** Software Developers? Perceptions of Productivity. A. N. Meyer, T. Fritz, G. C. Murphy, T. Zimmermann. In Proc. of the ACM SIGSOFT 22nd International Symposium on the Foundations of Software Engineering 2014 (FSE'14). 

**[Müller2013]** Stakeholders? Information Needs for Artifacts and their Dependencies in a Real World Context. S. Müller, T. Fritz. In Proc. of the IEEE Int?l Conference on Software Maintenance 2013 (ICSM'13).

**[Perry1994]**  People, organizations, and process improvement. D. Perry, N. Staudenmayer, and L. Votta. Software, 11(4):36?45, 1994. 

**[Singer2010]** An examination of software engineering work practices. J. Singer, T. Lethbridge, N. Vinson, and N. Anquetil. In CASCON First Decade High Impact Papers, (CASCON ?10).

**[Withings]** [http://www2.withings.com/us/en/products/pulse}(http://www2.withings.com/us/en/products/pulse)